{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI TICKET CLASSIFIER - COMPLETE ASSESMENT SOLUTION\n",
        "\n",
        "This script implements a complete ticket classification system with:\n",
        "1. Data setup and analysis\n",
        "2. NLP pipeline with text processing\n",
        "3. Machine Learning model training and evaluation\n",
        "4. Named Entity Recognition(NER)"
      ],
      "metadata": {
        "id": "3pQYelUmWOOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Everything we need"
      ],
      "metadata": {
        "id": "HKd0b_2fXLEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R2ii1B-0WJjP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "from nltk.tree import Tree\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download required NLTK data"
      ],
      "metadata": {
        "id": "mtLqn8khYnPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "  nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "  nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "  nltk.download('punkt_tab')\n",
        "\n",
        "try:\n",
        "  nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "  nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "  nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "  nltk.download('wordnet')\n",
        "\n",
        "try:\n",
        "  nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "  nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "try:\n",
        "  nltk.data.find('chunkers/maxent_ne_chunker')\n",
        "except LookupError:\n",
        "  nltk.download('maxent_ne_chunker')\n",
        "\n",
        "try:\n",
        "  nltk.data.find('corpora/words')\n",
        "except LookupError:\n",
        "  nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjITv9v2ZveX",
        "outputId": "de83cefa-488a-4ece-d797-f1848077fb55"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Ticket Classifier Class"
      ],
      "metadata": {
        "id": "qbJuIBHHeASI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TicketClassifier:\n",
        "  def __init__(self):   # javascript we use this and in python we use self\n",
        "     self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "     self.classifier = MultinomialNB()\n",
        "     self.lemmatizer = WordNetLemmatizer()\n",
        "     self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  def load_and_analyze_data(self, csv_file=None):\n",
        "    \"\"\"TASK 1: Load an analyze the ticket data\"\"\"\n",
        "    print(\"=== TASK 1: AUTOMATE TICKET CLASSIFICATION ===\")\n",
        "    print(\"Objective: Set up the dataset \\n\")\n",
        "\n",
        "    if csv_file:\n",
        "      try:\n",
        "        #Load actual CSV file\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        print(f\"Loaded {len(self.df)} tickets from {csv_file}\")\n",
        "\n",
        "        #Add Data validation\n",
        "        #check for missing values in ticket_text\n",
        "        if self.df['ticket_text'].isna().any():\n",
        "          print(\"Found missing ticket_text values, filling with default text\")\n",
        "          self.df['ticket_text'] = self.df['ticket_text'].fillna('No description provided')\n",
        "\n",
        "        #ensure all ticket_text entries are strings\n",
        "        self.df['ticket_text'] = self.df['ticket_text'].astype(str)\n",
        "\n",
        "        #fill any empty categories with empty string\n",
        "        self.df['category'] = self.df['category'].fillna('')\n",
        "\n",
        "\n",
        "      except FileNotFoundError:\n",
        "        print(\"Could not find csv file exception\")\n",
        "    else:\n",
        "      print(\"Could not find csv file\")\n",
        "\n",
        "\n",
        "    print(\"\\n Data Collection Complete:\")\n",
        "    print(f\" Total ticket loaded: {len(self.df)}\")\n",
        "\n",
        "    print(\"\\n 2. Quick check - Category Distribution:\")\n",
        "    category_counts = self.df['category'].value_counts()\n",
        "    print(f\" Technical: {category_counts.get('Technical', 0)} tickets\")\n",
        "    print(f\" Billing: {category_counts.get('Billing', 0)} tickets\")\n",
        "    print(f\" General: {category_counts.get('General', 0)} tickets\")\n",
        "\n",
        "    # Count unlabeled tickets\n",
        "    unlabeled_count = len(self.df[self.df['category'] == ''])\n",
        "    labeled_count = len(self.df[self.df['category'] != ''])\n",
        "\n",
        "    print(f\"\\n Unlabeled tickets: {unlabeled_count}\")\n",
        "    print(f\" Labeled tickets: {labeled_count}\")\n",
        "\n",
        "    #Separate labeled and unlabeled data\n",
        "    self.labeled_df = self.df[self.df['category'] != ''].copy()\n",
        "    self.unlabeled_df = self.df[self.df['category'] == ''].copy()\n",
        "\n",
        "    print(\"\\n Data Split:\")\n",
        "    print(f\" Labeled tickets: {len(self.labeled_df)}\")\n",
        "    print(f\" Unlabeled tickets: {len(self.unlabeled_df)}\\n\")\n",
        "\n",
        "    #Split labeled data: 10 training, 3 validation, 2 test\n",
        "\n",
        "    train_data, temp_data = train_test_split(self.labeled_df, test_size=5, random_state=42, stratify=self.labeled_df['category'])\n",
        "    val_data, test_data = train_test_split(temp_data, test_size=2, random_state=42)\n",
        "\n",
        "    self.train_data = train_data\n",
        "    self.val_data = val_data\n",
        "    self.test_data = test_data\n",
        "\n",
        "    print(f\" Training set: {len(self.train_data)} tickets\") #same thing in javascript as objt.length\n",
        "    print(f\" Validation set: {len(self.val_data)} tickets\")\n",
        "    print(f\" Test set: {len(self.test_data)} tickets\")\n",
        "\n",
        "    return self.df\n",
        "\n",
        "  def extract_named_entities(self, text):\n",
        "     \"\"\" Extract named entities using NLTK's NER\"\"\"\n",
        "     try:\n",
        "       tokens = word_tokenize(text)\n",
        "       pos_tag = pos_tag(tokens)\n",
        "       tree = ne_chunk(pos_tag)\n",
        "\n",
        "       entities = []\n",
        "       for chunk in tree:\n",
        "         if isinstance(chunk, Tree):\n",
        "           entities.append(' '.join([token for token, pos in chunk.leaves()]))\n",
        "       return entities\n",
        "\n",
        "     except:\n",
        "       return []\n",
        "\n",
        "  def nlp_pipeline(self, text):\n",
        "    \"\"\"  Task 2: Complete NBLP Pipeline for text processing\"\"\"\n",
        "    # Add a validation for empty/null text\n",
        "    if pd.isna(text) or not isinstance(text, str) or len(text.strip()) == 0:\n",
        "      return {\n",
        "          'processed_text': 'empty ticket',\n",
        "          'tokens': ['empty', 'ticket'],\n",
        "          'key_terms': ['empty', ' ticket'],\n",
        "          'entities': []\n",
        "      }\n",
        "\n",
        "    # 1. Convert to lowercase and remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
        "\n",
        "    # 2. Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 3. Remove stop words\n",
        "    tokens = [token for token in tokens if token not in self.stop_words and len(token) > 2]\n",
        "\n",
        "    # 4. Lemmatization\n",
        "    lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "\n",
        "    # handle any empty result\n",
        "    if not lemmatized_tokens:\n",
        "      lemmatized_tokens = ['short', 'text']\n",
        "\n",
        "    # 5. Extract Key terms using simple frequency and NER\n",
        "    processed_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    # ensure processed_text is not empty\n",
        "    if not processed_text.strip():\n",
        "      processed_text = 'short text'\n",
        "\n",
        "    # extract named entities\n",
        "    try:\n",
        "      entities = self.extract_named_entities(text)\n",
        "\n",
        "    except:\n",
        "      entities = []\n",
        "\n",
        "    # Combine processed text with entities\n",
        "    key_terms = lemmatized_tokens[:5] + entities # top 5 key terms + entities\n",
        "\n",
        "    return {\n",
        "        'processed_text': processed_text,\n",
        "        'tokens': lemmatized_tokens,\n",
        "        'key_terms': key_terms[:3] if key_terms else ['short', 'text'],\n",
        "        'entities': entities\n",
        "    }\n",
        "\n",
        "  def build_nlp_pipeline(self):\n",
        "    \"\"\"Task 2: Build NLP pipeline and process  all tickets\"\"\"\n",
        "    print(\"\\n === Task 2: NLP Pipeline == \\n\")\n",
        "    print(\"Text Cleanup Steps:\")\n",
        "    print(\"1. Remove stop words (eg, 'the', 'this')\")\n",
        "    print(\"2. Lemmatize words (eg 'crashed' -> 'crash')\")\n",
        "    print(\"3. Tokenize text into words\")\n",
        "    print(\"4. Extract named entities and key terms \\n\")\n",
        "\n",
        "    #process all tickets\n",
        "    processed_tickets = []\n",
        "    print(\"Processing tickets with NLP pipeline\")\n",
        "    for idx, row in self.df.iterrows():\n",
        "      processed = self.nlp_pipeline(row['ticket_text'])\n",
        "      processed_tickets.append({\n",
        "          'ticket_id': row['ticket_id'],\n",
        "          'original_text': row['ticket_text'],\n",
        "          'processed_text': processed['processed_text'],\n",
        "          'key_terms': processed['key_terms'],\n",
        "          'entities': processed['entities'],\n",
        "          'category': row['category']\n",
        "\n",
        "      })\n",
        "\n",
        "      if idx == 0:\n",
        "        print(f\"Example Ticket {row['ticket_id']}\")\n",
        "        print(f\"Original {row['ticket_text']}\")\n",
        "        print(f\"Key Terms {processed['key_terms']}\")\n",
        "        print(f\"Entities {processed['entities']}\")\n",
        "\n",
        "    self.processed_data = pd.DataFrame(processed_tickets)\n",
        "    print(f\"\\n NLP pipeline applied to all {len(self.df)} tickets\")\n",
        "\n",
        "    return self.processed_data\n",
        "\n",
        "  def train_and_evaluate_model(self):\n",
        "    \"\"\" Task 3: Train and evaluate the ML model\"\"\"\n",
        "    print(\"\\n == Task 3: Train and evaluate ML Model\")\n",
        "    print(\"Objective: Train and test the model \\n\")\n",
        "\n",
        "    # 1. Algorithm Selection\n",
        "    print(\"1. Algorithm Selection:\")\n",
        "    print(\"Primary: Naive Bayes - fast and effective for text classification\")\n",
        "    print(\"Secondary: Logistic Regression - good baseline for comparison\")\n",
        "    print(\"Clustering: K-Means for unlabeled ticket analysis \\n\")\n",
        "\n",
        "    #Prepare training data\n",
        "    train_texts = self.train_data['ticket_text'].tolist()\n",
        "    train_labels = self.train_data['category'].tolist()\n",
        "\n",
        "    # 2. Train the model\n",
        "    print(\"2. Model Training\")\n",
        "\n",
        "    #processed training text through NLP pipeline\n",
        "    processed_train_texts = []\n",
        "    for text in train_texts:\n",
        "      processed = self.nlp_pipeline(text)\n",
        "      processed_train_texts.append(processed['processed_text'])\n",
        "\n",
        "    #Add validation: check for empty processed texts\n",
        "    if any(not text.strip() for text in processed_train_texts):\n",
        "      print(\" Warning: some tickets produced empty text after processing\")\n",
        "      processed_train_texts = [text if text.strip() else 'empty ticket' for text in processed_train_texts]\n",
        "\n",
        "    # Vectorize the text (basically doing [1, 2] -> [[1], [2]])\n",
        "    X_train = self.vectorizer.fit_transform(processed_train_texts)\n",
        "    y_train = train_labels\n",
        "\n",
        "    # Train Naive Bayes\n",
        "    self.classifier.fit(X_train, y_train)\n",
        "    print(f\"  Naive Bayes trained on {len(train_texts)} tickets\")\n",
        "\n",
        "    # Train Logistic Regression for comparison\n",
        "    self.lr_classifier = LogisticRegression(random_state=42)\n",
        "    self.lr_classifier.fit(X_train, y_train)\n",
        "    print(f\" Logistic Regression trained on {len(train_texts)} tickets\")\n",
        "\n",
        "    # 3. Validation Testing\n",
        "    print(\"\\n 3. Validation Testing:\")\n",
        "    val_texts = self.val_data['ticket_text'].tolist()\n",
        "    val_labels = self.val_data['category'].tolist()\n",
        "\n",
        "    processed_val_texts = []\n",
        "    for text in val_texts:\n",
        "      processed = self.nlp_pipeline(text)\n",
        "      processed_val_texts.append(processed['processed_text'])\n",
        "\n",
        "    X_val = self.vectorizer.transform(processed_val_texts)\n",
        "    val_predictions = self.classifier.predict(X_val)\n",
        "    val_accuracy = accuracy_score(val_labels, val_predictions)\n",
        "\n",
        "    print(f\" Validation Accuracy: {val_accuracy:.2%}\")\n",
        "    print(f\" Validation Results: {sum(val_predictions == val_labels)}/{len(val_labels)} correct\")\n",
        "\n",
        "    # 4. Test Set Evaluation\n",
        "    print(\"\\n 4. Test Set Evaluation\")\n",
        "    test_texts = self.test_data['ticket_text'].tolist()\n",
        "    test_labels = self.test_data['category'].tolist()\n",
        "\n",
        "    processed_test_texts = []\n",
        "    for text in test_texts:\n",
        "      processed = self.nlp_pipeline(text)\n",
        "      processed_test_texts.append(processed['processed_text'])\n",
        "\n",
        "    X_test = self.vectorizer.transform(processed_test_texts)\n",
        "    test_predictions = self.classifier.predict(X_test)\n",
        "    test_accuracy = accuracy_score(test_labels, test_predictions)\n",
        "\n",
        "    print(f\" Test Accuracy: {test_accuracy:.2%}\")\n",
        "    print(f\" Test Results: {sum(test_predictions == test_labels)}/{len(test_labels)} correct\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w_dM9G8UeFNZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invoking our class"
      ],
      "metadata": {
        "id": "iaIyiM1oq4MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize Ticket Classifier\n",
        "classifier = TicketClassifier()\n",
        "\n",
        "# Task 1: Load and analyze data\n",
        "classifier.load_and_analyze_data(csv_file='/content/tickets.csv')\n",
        "\n",
        "# Task 2: Build NLP Pipeline (remember nlp_pipeline is being called inside this function)\n",
        "classifier.build_nlp_pipeline()\n",
        "\n",
        "# Task 3: Train and evaluate model\n",
        "results = classifier.train_and_evaluate_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbixU_Y0q9Os",
        "outputId": "b386bd9c-13f9-4704-9fc7-79600948dbb0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TASK 1: AUTOMATE TICKET CLASSIFICATION ===\n",
            "Objective: Set up the dataset \n",
            "\n",
            "Loaded 20 tickets from /content/tickets.csv\n",
            "\n",
            " Data Collection Complete:\n",
            " Total ticket loaded: 20\n",
            "\n",
            " 2. Quick check - Category Distribution:\n",
            " Technical: 5 tickets\n",
            " Billing: 5 tickets\n",
            " General: 5 tickets\n",
            "\n",
            " Unlabeled tickets: 5\n",
            " Labeled tickets: 15\n",
            "\n",
            " Data Split:\n",
            " Labeled tickets: 15\n",
            " Unlabeled tickets: 5\n",
            "\n",
            " Training set: 10 tickets\n",
            " Validation set: 3 tickets\n",
            " Test set: 2 tickets\n",
            "\n",
            " === Task 2: NLP Pipeline == \n",
            "\n",
            "Text Cleanup Steps:\n",
            "1. Remove stop words (eg, 'the', 'this')\n",
            "2. Lemmatize words (eg 'crashed' -> 'crash')\n",
            "3. Tokenize text into words\n",
            "4. Extract named entities and key terms \n",
            "\n",
            "Processing tickets with NLP pipeline\n",
            "Example Ticket 1\n",
            "Original App crashes whenever I try to open it.\n",
            "Key Terms ['app', 'crash', 'whenever']\n",
            "Entities []\n",
            "\n",
            " NLP pipeline applied to all 20 tickets\n",
            "\n",
            " == Task 3: Train and evaluate ML Model\n",
            "Objective: Train and test the model \n",
            "\n",
            "1. Algorithm Selection:\n",
            "Primary: Naive Bayes - fast and effective for text classification\n",
            "Secondary: Logistic Regression - good baseline for comparison\n",
            "Clustering: K-Means for unlabeled ticket analysis \n",
            "\n",
            "2. Model Training\n",
            "  Naive Bayes trained on 10 tickets\n",
            " Logistic Regression trained on 10 tickets\n",
            "\n",
            " 3. Validation Testing:\n",
            " Validation Accuracy: 33.33%\n",
            " Validation Results: 1/3 correct\n",
            "\n",
            " 4. Test Set Evaluation\n",
            " Test Accuracy: 50.00%\n",
            " Test Results: 1/2 correct\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI TICKET CLASSIFIER - COMPLETE ASSESMENT SOLUTION\n",
        "\n",
        "This script implements a complete ticket classification system with:\n",
        "1. Data setup and analysis\n",
        "2. NLP pipeline with text processing\n",
        "3. Machine Learning model training and evaluation\n",
        "4. Named Entity Recognition(NER)"
      ],
      "metadata": {
        "id": "3pQYelUmWOOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Everything we need"
      ],
      "metadata": {
        "id": "HKd0b_2fXLEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R2ii1B-0WJjP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "from nltk.tree import Tree\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download required NLTK data"
      ],
      "metadata": {
        "id": "mtLqn8khYnPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "  nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "  nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "  nltk.download('punkt_tab')\n",
        "\n",
        "try:\n",
        "  nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "  nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "  nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "  nltk.download('wordnet')\n",
        "\n",
        "try:\n",
        "  nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "  nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "try:\n",
        "  nltk.data.find('chunkers/maxent_ne_chunker')\n",
        "except LookupError:\n",
        "  nltk.download('maxent_ne_chunker')\n",
        "\n",
        "try:\n",
        "  nltk.data.find('corpora/words')\n",
        "except LookupError:\n",
        "  nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjITv9v2ZveX",
        "outputId": "9bd33d91-c90d-47dc-fd75-024446a90001"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Ticket Classifier Class"
      ],
      "metadata": {
        "id": "qbJuIBHHeASI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TicketClassifier:\n",
        "  def __init__(self):   # javascript we use this and in python we use self\n",
        "     self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "     self.classifier = MultinomialNB()\n",
        "     self.lemmatizer = WordNetLemmatizer()\n",
        "     self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  def load_and_analyze_data(self, csv_file=None):\n",
        "    \"\"\"TASK 1: Load an analyze the ticket data\"\"\"\n",
        "    print(\"=== TASK 1: AUTOMATE TICKET CLASSIFICATION ===\")\n",
        "    print(\"Objective: Set up the dataset \\n\")\n",
        "\n",
        "    if csv_file:\n",
        "      try:\n",
        "        #Load actual CSV file\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        print(f\"Loaded {len(self.df)} tickets from {csv_file}\")\n",
        "\n",
        "        #Add Data validation\n",
        "        #check for missing values in ticket_text\n",
        "        if self.df['ticket_text'].isna().any():\n",
        "          print(\"Found missing ticket_text values, filling with default text\")\n",
        "          self.df['ticket_text'] = self.df['ticket_text'].fillna('No description provided')\n",
        "\n",
        "        #ensure all ticket_text entries are strings\n",
        "        self.df['ticket_text'] = self.df['ticket_text'].astype(str)\n",
        "\n",
        "        #fill any empty categories with empty string\n",
        "        self.df['category'] = self.df['category'].fillna('')\n",
        "\n",
        "\n",
        "      except FileNotFoundError:\n",
        "        print(\"Could not find csv file exception\")\n",
        "    else:\n",
        "      print(\"Could not find csv file\")\n",
        "\n",
        "\n",
        "    print(\"\\n Data Collection Complete:\")\n",
        "    print(f\" Total ticket loaded: {len(self.df)}\")\n",
        "\n",
        "    print(\"\\n 2. Quick check - Category Distribution:\")\n",
        "    category_counts = self.df['category'].value_counts()\n",
        "    print(f\" Technical: {category_counts.get('Technical', 0)} tickets\")\n",
        "    print(f\" Billing: {category_counts.get('Billing', 0)} tickets\")\n",
        "    print(f\" General: {category_counts.get('General', 0)} tickets\")\n",
        "\n",
        "    # Count unlabeled tickets\n",
        "    unlabeled_count = len(self.df[self.df['category'] == ''])\n",
        "    labeled_count = len(self.df[self.df['category'] != ''])\n",
        "\n",
        "    print(f\"\\n Unlabeled tickets: {unlabeled_count}\")\n",
        "    print(f\" Labeled tickets: {labeled_count}\")\n",
        "\n",
        "    #Separate labeled and unlabeled data\n",
        "    self.labeled_df = self.df[self.df['category'] != ''].copy()\n",
        "    self.unlabeled_df = self.df[self.df['category'] == ''].copy()\n",
        "\n",
        "    print(\"\\n Data Split:\")\n",
        "    print(f\" Labeled tickets: {len(self.labeled_df)}\")\n",
        "    print(f\" Unlabeled tickets: {len(self.unlabeled_df)}\\n\")\n",
        "\n",
        "    #\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w_dM9G8UeFNZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invoking our class"
      ],
      "metadata": {
        "id": "iaIyiM1oq4MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = TicketClassifier()\n",
        "\n",
        "classifier.load_and_analyze_data(csv_file='/content/tickets.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbixU_Y0q9Os",
        "outputId": "a8d6d33c-b36e-41d6-8b50-65abb0ec6727"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TASK 1: AUTOMATE TICKET CLASSIFICATION ===\n",
            "Objective: Set up the dataset \n",
            "\n",
            "Loaded 20 tickets from /content/tickets.csv\n",
            "\n",
            " Data Collection Complete:\n",
            " Total ticket loaded: 20\n",
            "\n",
            " 2. Quick check - Category Distribution:\n",
            " Technical: 5 tickets\n",
            " Billing: 5 tickets\n",
            " General: 5 tickets\n",
            "\n",
            " Unlabeled tickets: 5\n",
            " Labeled tickets: 15\n",
            "\n",
            " Data Split:\n",
            " Labeled tickets: 15\n",
            " Unlabeled tickets: 5\n",
            "\n"
          ]
        }
      ]
    }
  ]
}